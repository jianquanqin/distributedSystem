# 1 分布式介绍

分布式其实就是将相同或相关的程序运行在多台计算机上，从而实现特定目标的一种计算方式

产生分布式的最主要驱动力量，是我们对于性能、可用性及可扩展性的不懈追求

## 1.1 分布式发展史

### 1.1.1 单机模式

应用程序和数据均部署在一台电脑或者服务器上，由单机完成所有处理。瓶颈：性能受限、存在单点失效

### 1.1.2 游击队模式

数据并行或数据分布式。核心原理是每台计算机上执行相同的程序，将数据进行拆分放到不同的计算机上进行计算。

采用消息共享模式使用多台计算机并行运行或执行多项任务

两个步骤：第一，将数据和应用分离部署到不同的服务器上。第二，将数据拆分到更多的服务器中

一些问题与解决方案：1. 任务需要转发到多个服务器，需要负载均衡；2. 请求较大时，数据库的IO访问成为瓶颈，需要读写分离以及之后的数据一致性；3. 热点数据导致数据库访问频繁，压力增大，需要将热点数据加入缓存中，以减轻数据库压力和提升查询效率

瓶颈：复杂问题整体效率不高，即对提升单个任务的执行性能及降低延时无效

### 1.1.3 集团军模式

任务并行和任务分布式。

任务并行指的是，将单个复杂的任务拆分为多个子任务，从而使得多个子任务可以在不同的计算机上并行执行

问题：集团军模式在提供了更好的性能、扩展性、可维护性的同时，也带来了设计上的复杂性问题

## 1.2 分布式系统指标

性能、资源、可用性和可扩展性

### 1.2.1 行能

包括吞吐量（Throughput）、响应时间（Response Time）和完成时间

吞吐量：吞吐量指标有 QPS（Queries Per Second）、TPS（Transactions Per Second）和 BPS（Bits Per Second）

### 1.2.2 资源占用

资源占用指的是，一个系统提供正常能力需要占用的硬件资源，比如 CPU、内存、硬盘等。

### 1.2.3 可用性

系统的可用性可以用系统停止服务的时间与总的时间之比衡量

### 1.2.4 可扩展性

可扩展性，指的是分布式系统通过扩展集群机器规模提高系统性能 (吞吐量、响应时间、 完成时间)、存储容量、计算能力的特性，是分布式系统的特有性质

区块链。区块链的吞吐量比较低，比特币的 TPS 只有 7 次每秒，单平均一次交易的确认就需要 10 分钟左右，因此吞吐量和完成时间通常是区块链系统设计者的首要目标

![img](https://static001.geekbang.org/resource/image/24/6a/24e536a1903c871b3ffd9e70eb7f836a.jpg?wh=2362*1556)

## 1.3 分布式场景

![img](https://static001.geekbang.org/resource/image/62/e9/62b5acf80f49fe8d1a8b229df36f00e9.jpg?wh=3298*2354)

# 2 分布式协调与同步

## 2.1 分布式互斥

在分布式系统里，排他性的资源访问方式，叫作分布式互斥（Distributed Mutual Exclusion），而这种被互斥访问的共享资源就叫作临界资源（Critical Resource）。

### 2.1.1 集中式算法

引入协调者分发对临界资源的访问权。问题：协调者的性能瓶颈与单点故障问题

### 2.1.2 分布式算法

前提，每个程序使用临界资源时都需要获得其他进程的同意。设系统中有n个进程，某个进程需要发送n-1条信息并获得回复才能使用。当要使用的程序越多时，最多需要 2n(n-1)次信息交互。指数级增加，通信成本高昂，消息体：请求的资源、请求者的 ID，以及发起请求的时间

分布式算法核心：“先到先得”以及“投票全票通过”的机制

问题：当系统内需要访问临界资源的程序增多时，容易产生“信令风暴”，也就是程序收到的请求完全超过了自己的处理能力，而导致自己正常的业务无法开展。一旦某一程序发生故障，无法发送同意消息，那么其他程序均处在等待回复的状态中，使得整个系统处于停滞状态，导致整个系统不可用。所以，相对于集中式算法的协调者故障，分布式算法的可用性更低。

分布式算法适合节点数目少且变动不频繁的系统，且由于每个程序均需通信交互，因此适合 P2P 结构的系统。

改进办法：检查并忽略故障节点

### 2.1.3 令牌环算法

有程序构成一个环结构，令牌按照顺时针（或逆时针）方向在程序之间传递，收到令牌的程序有权访问临界资源，访问完成后将令牌传送到下一个程序；若该程序不需要访问临界资源，则直接把令牌传送给下一个程序。

令牌环算法的公平性高，在改进单点故障后，稳定性也很高，适用于系统规模较小，并且系统中每个程序使用临界资源的频率高且使用时间比较短的场景。

![img](https://static001.geekbang.org/resource/image/42/c6/4210e133d9d94ea22917db55458c11c6.png?wh=1548*1432)

## 2.2 分布式选举

分布式选举的算法

### 2.2.1 长者为大：Bully 算法

选举原则是“长者”为大，即在所有活着的节点中，选取 ID 最大的节点作为主节点。假设条件是，集群中每个节点均知道其他节点的 ID

```
Bully 算法在选举过程中，需要用到以下 3 种消息：

Election 消息，用于发起选举；
Alive 消息，对 Election 消息的应答；
Victory 消息，竞选成功的主节点向其他节点发送的宣誓主权的消息。
```

1. 集群中每个节点判断自己的 ID 是否为当前活着的节点中 ID 最大的，如果是，则直接向其他节点发送 Victory 消息，宣誓自己的主权；
2. 如果自己不是当前活着的节点中 ID 最大的，则向比自己 ID 大的所有节点发送 Election 消息，并等待其他节点的回复；
3. 若在给定的时间范围内，本节点没有收到其他节点回复的 Alive 消息，则认为自己成为主节点，并向其他节点发送 Victory 消息，宣誓自己成为主节点；
4. 若接收到来自比自己 ID 大的节点的 Alive 消息，则等待其他节点发送 Victory 消息；若本节点收到比自己 ID 小的节点发送的 Election 消息，则回复一个 Alive 消息，告知其他节点，我比你大，重新选举。

这种算法的优点是，选举速度快、算法复杂度低、简单易实现

缺点在于，需要每个节点有全局的节点信息，因此额外信息存储较多；其次，任意一个比当前主节点 ID 大的新节点或节点故障后恢复加入集群的时候，都可能会触发重新选举，成为新的主节点，如果该节点频繁退出、加入集群，就会导致频繁切主。

### 2.2.2 民主投票：Raft算法

核心思想是“少数服从多数”

采用 Raft 算法选举，集群节点的角色有 3 种：Leader，即主节点，同一时刻只有一个 Leader，负责协调和管理其他节点；Candidate，即候选者，每一个节点都可以成为 Candidate，节点在该角色下才可以被选为新的 Leader；Follower，Leader 的跟随者，不可以发起选举。

Raft 选举的流程，可以分为以下几步：

1. 初始化时，所有节点均为 Follower 状态。
2. 开始选主时，所有节点的状态由 Follower 转化为 Candidate，并向其他节点发送选举请求。
3. 其他节点根据接收到的选举请求的先后顺序，回复是否同意成为主。这里需要注意的是，在每一轮选举中，一个节点只能投出一张票。
4. 若发起选举请求的节点获得超过一半的投票，则成为主节点，其状态转化为 Leader，其他节点的状态则由 Candidate 降为 Follower。Leader 节点与 Follower 节点之间会定期发送心跳包，以检测主节点是否活着。
5. 当 Leader 节点的任期到了，即发现其他服务器开始下一轮选主周期时，Leader 节点的状态由 Leader 降级为 Follower，进入新一轮选主。

Google 开源的 Kubernetes，擅长容器管理与调度，为了保证可靠性，通常会部署 3 个节点用于数据备份。Kubernetes 的选主采用的是开源的 etcd 组件。而，etcd 的集群管理器 etcds，是一个高可用、强一致性的服务发现存储仓库，就是采用了 Raft 算法来实现选主和一致性的。

Raft 算法具有选举速度快、算法复杂度低、易于实现的优点；缺点是，它要求系统内每个节点都可以相互通信，且需要获得过半的投票数才能选主成功，因此通信量大。该算法选举稳定性比 Bully 算法好，这是因为当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点获得投票数过半，才会导致切主

### 2.2.3 优先级民主投票：ZAB算法

相较于 Raft 算法的投票机制，ZAB 算法增加了通过节点 ID 和数据 ID 作为参考进行选主，节点 ID 和数据 ID 越大，表示数据越新，优先成为主

ZAB 算法选举时，集群中每个节点拥有 3 种角色：Leader，主节点；Follower，跟随者节点；Observer，观察者，无投票权

选举过程中，集群中的节点拥有 4 个状态：

1. Looking 状态，即选举状态。当节点处于该状态时，它会认为当前集群中没有 Leader，因此自己进入选举状态。
2. Leading 状态，即领导者状态，表示已经选出主，且当前节点为 Leader。
3. Following 状态，即跟随者状态，集群中已经选出主后，其他非主节点状态更新为 Following，表示对 Leader 的追随。
4. Observing 状态，即观察者状态，表示当前节点为 Observer，持观望态度，没有投票权和选举权。

ZAB 选举算法的核心是“少数服从多数，ID 大的节点优先成为主”

投票过程中，每个节点都有一个唯一的三元组 (server_id, server_zxID, epoch)，其中 server_id 表示本节点的唯一 ID；server_zxID 表示本节点存放的数据 ID，数据 ID 越大表示数据越新，选举权重越大；epoch 表示当前选取轮数，一般用逻辑时钟表示。

ZAB 算法性能高，对系统无特殊要求，采用广播方式发送信息，若节点中有 n 个节点，每个节点同时广播，则集群中信息量为 n*(n-1) 个消息，容易出现广播风暴；且除了投票，还增加了对比节点 ID 和数据 ID，这就意味着还需要知道所有节点的 ID 和数据 ID，所以选举时间相对较长。但该算法选举稳定性比较好，当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点数据 ID 和节点 ID 最大，且获得投票数过半，才会导致切主。

ZAB 算法性能高，对系统无特殊要求，采用广播方式发送信息，若节点中有 n 个节点，每个节点同时广播，则集群中信息量为 n*(n-1) 个消息，容易出现广播风暴；且除了投票，还增加了对比节点 ID 和数据 ID，这就意味着还需要知道所有节点的 ID 和数据 ID，所以选举时间相对较长。但该算法选举稳定性比较好，当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点数据 ID 和节点 ID 最大，且获得投票数过半，才会导致切主

![img](https://static001.geekbang.org/resource/image/e4/7e/e411f24b0b03991ad761134dfc3dff7e.jpg?wh=3400*2142)

![img](https://static001.geekbang.org/resource/image/04/bd/04dfd1e4b8a1558fcbfa1bb8a9b077bd.png?wh=833*873)

## 2.3  分布式共识

### 2.3.1 PoW 机制

假设每个节点会划分多个区块用于记录用户交易，PoW 算法获取记账权的原理是：利用区块的 index、前一个区块的哈希值、交易的时间戳、区块数据和 nonce 值，通过 SHA256 哈希算法计算出一个哈希值，并判断前 k 个值是否都为 0。如果不是，则递增 nonce 值，重新按照上述方法计算；如果是，则本次计算的哈希值为要解决的题目的正确答案。谁最先计算出正确答案，谁就获得这个区块的记账权。

其重心在于货币，比特币大约 10min 才会产生一个区块，区块的大小也只有 1MB，仅能够包含 3000～4000 笔交易，平均每秒只能够处理 5~7（个位数）笔交易。

PoW 机制的缺点也很明显，共识达成的周期长、效率低，资源消耗大。

### 2.3.2 PoS

所谓的权益，就是每个节点占有货币的数量和时间，而货币就是节点所获得的奖励。

PoS 算法中持币越多或持币越久，币龄就会越高，持币人就越容易挖到区块并得到激励，而持币少的人基本没有机会，这样整个系统的安全性实际上会被持币数量较大的一部分人掌握，容易出现垄断现象。

### 2.3.3 DPoS

DPoS 算法的原理，类似股份制公司的董事会制度，普通股民虽然拥有股权，但进不了董事会，他们可以投票选举代表（受托人）代他们做决策。DPoS 是由被社区选举的可信帐户（受托人，比如得票数排行前 101 位）来拥有记账权。

但是，在 DPoS 中，由于大多数持币人通过受托人参与投票，投票的积极性并不高；且一旦出现故障节点，DPoS 无法及时做出应对，导致安全隐患。

![img](https://static001.geekbang.org/resource/image/b2/29/b2a43f0e0239f083a2c89db7bce2f729.jpg?wh=2510*1197)

### 2.3.4 NPoS

## 2.3 分布式事务

事务（Transaction）提供一种机制，将包含一系列操作的工作序列纳入到一个不可分割的执行单元。只有所有操作均被正确执行才能提交事务；任意一个操作失败都会导致整个事务回滚（Rollback）到之前状态，即所有操作均被取消。简单来说，事务提供了一种机制，使得工作要么全部都不做，要么完全被执行，即 all or nothing。

而事务具备四大基本特征 ACID：原子性，一致性，隔离性，持久性

如何实现分布式事务？

### 2.3.1 基于 XA 协议的二阶段提交协议方法

XA 是一个分布式事务协议，规定了事务管理器和资源管理器接口。因此，XA 协议包括事务管理器和本地资源管理器两个部分。事务管理器相当于协调者，负责各个本地资源的提交和回滚；而资源管理器就是分布式事务的参与者，通常由数据库实现，比如 Oracle、DB2 等商业数据库都实现了 XA 接口。

两阶段提交协议的执行过程，分为投票（Voting）和提交（Commit）两个阶段。

第一阶段投票：在这一阶段，协调者（Coordinator，即事务管理器）会向事务的参与者（Cohort，即本地资源管理器）发起执行操作的 CanCommit 请求，并等待参与者的响应。参与者接收到请求后，会执行请求中的事务操作，将操作信息记录到事务日志中但不提交（即不会修改数据库中的数据），待参与者执行成功，则向协调者发送“Yes”消息，表示同意操作；若不成功，则发送“No”消息，表示终止操作

当所有的参与者都返回了操作结果（Yes 或 No 消息）后，系统进入了第二阶段提交阶段（也可以称为，执行阶段）。在提交阶段，协调者会根据所有参与者返回的信息向参与者发送 DoCommit（提交）或 DoAbort（取消）指令。

若协调者从参与者那里收到的都是“Yes”消息，则向参与者发送“DoCommit”消息。参与者收到“DoCommit”消息后，完成剩余的操作（比如修改数据库中的数据）并释放资源（整个事务过程中占用的资源），然后向协调者返回“HaveCommitted”消息；若协调者从参与者收到的消息中包含“No”消息，则向所有参与者发送“DoAbort”消息。此时投票阶段发送“Yes”消息的参与者，则会根据之前执行操作时的事务日志对操作进行回滚，就好像没有执行过请求操作一样，然后所有参与者会向协调者发送“HaveCommitted”消息；

协调者接收到来自所有参与者的“HaveCommitted”消息后，就意味着整个事务结束了。

主要有以下三个问题：

1. 同步阻塞问题：二阶段提交算法在执行过程中，所有参与节点都是事务阻塞型的。也就是说，当本地资源管理器占有临界资源时，其他资源管理器如果要访问同一临界资源，会处于阻塞状态。因此，基于 XA 的二阶段提交协议不支持高并发场景。
2. 单点故障问题：该算法类似于集中式算法，一旦事务管理器发生故障，整个系统都处于停滞状态。尤其是在提交阶段，一旦事务管理器发生故障，资源管理器会由于等待管理器的消息，而一直锁定事务资源，导致整个系统被阻塞。
3. 数据不一致问题：在提交阶段，当协调者向所有参与者发送“DoCommit”请求时，如果发生了局部网络异常，或者在发送提交请求的过程中协调者发生了故障，就会导致只有一部分参与者接收到了提交请求并执行提交操作，但其他未接到提交请求的那部分参与者则无法执行事务提交。于是整个分布式系统便出现了数据不一致的问题。

### 2.3.2 三阶段提交协议方法

是对二阶段提交（2PC）的改进。为了更好地处理两阶段提交的同步阻塞和数据不一致问题，三阶段提交引入了超时机制和准备阶段

与 2PC 只是在协调者引入超时机制不同，3PC 同时在协调者和参与者中引入了超时机制。如果协调者或参与者在规定的时间内没有接收到来自其他节点的响应，就会根据当前的状态选择提交或者终止整个事务，从而减少了整个集群的阻塞时间，在一定程度上减少或减弱了 2PC 中出现的同步阻塞问题。

在第一阶段和第二阶段中间引入了一个准备阶段，或者说把 2PC 的投票阶段一分为二，也就是在提交阶段之前，加入了一个预提交阶段。在预提交阶段尽可能排除一些不一致的情况，保证在最后提交之前各参与节点的状态是一致的

第一，CanCommit 阶段

协调者向参与者发送请求操作（CanCommit 请求），询问参与者是否可以执行事务提交操作，然后等待参与者的响应；参与者收到 CanCommit 请求之后，回复 Yes，表示可以顺利执行事务；否则回复 No。3PC 的 CanCommit 阶段与 2PC 的 Voting 阶段相比：类似之处在于：协调者均需要向参与者发送请求操作（CanCommit 请求），询问参与者是否可以执行事务提交操作，然后等待参与者的响应。参与者收到 CanCommit 请求之后，回复 Yes，表示可以顺利执行事务；否则回复 No。不同之处在于，在 2PC 中，在投票阶段，若参与者可以执行事务，会将操作信息记录到事务日志中但不提交，并返回结果给协调者。但在 3PC 中，在 CanCommit 阶段，参与者仅会判断是否可以顺利执行事务，并返回结果。而操作信息记录到事务日志但不提交的操作由第二阶段预提交阶段执行

第二，PreCommit 阶段。协调者根据参与者的回复情况，来决定是否可以进行 PreCommit 操作（预提交阶段）。如果所有参与者回复的都是“Yes”，那么协调者就会执行事务的预执行：协调者向参与者发送 PreCommit 请求，进入预提交阶段。参与者接收到 PreCommit 请求后执行事务操作，并将 Undo 和 Redo 信息记录到事务日志中。如果参与者成功执行了事务操作，则返回 ACK 响应，同时开始等待最终指令。假如任何一个参与者向协调者发送了“No”消息，或者等待超时之后，协调者都没有收到参与者的响应，就执行中断事务的操作：协调者向所有参与者发送“Abort”消息。参与者收到“Abort”消息之后，或超时后仍未收到协调者的消息，执行事务的中断操作。

第三，DoCommit 阶段。DoCmmit 阶段进行真正的事务提交，根据 PreCommit 阶段协调者发送的消息，进入执行提交阶段或事务中断阶段。执行提交阶段：若协调者接收到所有参与者发送的 Ack 响应，则向所有参与者发送 DoCommit 消息，开始执行阶段。参与者接收到 DoCommit 消息之后，正式提交事务。完成事务提交之后，释放所有锁住的资源，并向协调者发送 Ack 响应。协调者接收到所有参与者的 Ack 响应之后，完成事务。事务中断阶段：协调者向所有参与者发送 Abort 请求。参与者接收到 Abort 消息之后，利用其在 PreCommit 阶段记录的 Undo 信息执行事务的回滚操作，释放所有锁住的资源，并向协调者发送 Ack 消息。协调者接收到参与者反馈的 Ack 消息之后，执行事务的中断，并结束事务。

3PC 协议在协调者和参与者均引入了超时机制。即当参与者在预提交阶段向协调者发送 Ack 消息后，如果长时间没有得到协调者的响应，在默认情况下，参与者会自动将超时的事务进行提交，从而减少整个集群的阻塞时间，在一定程度上减少或减弱了 2PC 中出现的同步阻塞问题。但三阶段提交仍然存在数据不一致的情况，比如在 PreCommit 阶段，部分参与者已经接受到 ACK 消息进入执行阶段，但部分参与者与协调者网络不通，导致接收不到 ACK 消息，此时接收到 ACK 消息的参与者会执行任务，未接收到 ACK 消息且网络不通的参与者无法执行任务，最终导致数据不一致。

### 2.3.3 基于消息的最终一致性方法

上述两种方法都存在两个共同的缺点，一是，同步执行，性能差；二是，数据不一致问题。为了解决这两个问题，通过分布式消息来确保事务最终一致性的方案便出现了

基于分布式消息的最终一致性方案的事务处理，引入了一个消息中间件（在本案例中，我们采用 Message Queue，MQ，消息队列），用于在多个应用之间进行消息传递。实际使用中，阿里就是采用 RocketMQ 机制来支持消息事务。

![img](https://static001.geekbang.org/resource/image/d9/a4/d9b2d32660e49a4ea613871337b570a4.png?wh=919*502)

1. 订单系统把订单消息发给消息中间件，消息状态标记为“待确认”。

2. 消息中间件收到消息后，进行消息持久化操作，即在消息存储系统中新增一条状态为“待发送”的消息。
3. 消息中间件返回消息持久化结果（成功 / 失败），订单系统根据返回结果判断如何进行业务操作。失败，放弃订单，结束（必要时向上层返回失败结果）；
4. 成功，则创建订单。订单操作完成后，把操作结果（成功 / 失败）发送给消息中间件。
5. 消息中间件收到业务操作结果后，根据结果进行处理：失败，删除消息存储中的消息，结束；成功，则更新消息存储中的消息状态为“待发送（可发送）”，并执行消息投递。
6. 如果消息状态为“可发送”，则 MQ 会将消息发送给支付系统，表示已经创建好订单，需要对订单进行支付。支付系统也按照上述方式进行订单支付操作。
7. 订单系统支付完成后，会将支付消息返回给消息中间件，中间件将消息传送给订单系统。若支付失败，则订单操作失败，订单系统回滚到上一个状态，MQ 中相关消息将被删除；若支付成功，则订单系统再调用库存系统，进行出货操作，操作流程与支付系统类似。

在上述过程中，可能会产生如下异常情况，其对应的解决方案为：

1. 订单消息未成功存储到 MQ 中，则订单系统不执行任何操作，数据保持一致；
2. MQ 成功将消息发送给支付系统（或仓库系统），但是支付系统（或仓库系统）操作成功的 ACK 消息回传失败（由于通信方面的原因），导致订单系统与支付系统（或仓库系统）数据不一致，此时 MQ 会确认各系统的操作结果，删除相关消息，支付系统（或仓库系统）操作回滚，使得各系统数据保持一致；
3. MQ 成功将消息发送给支付系统（或仓库系统），但是支付系统（或仓库系统）操作成功的 ACK 消息回传成功，订单系统操作后的最终结果（成功或失败）未能成功发送给 MQ，此时各系统数据可能不一致，MQ 也需确认各系统的操作结果，若数据一致，则更新消息；若不一致，则回滚操作、删除消息。

![img](https://static001.geekbang.org/resource/image/9c/2b/9c789a486aa8df6d9d12182b953a862b.jpg?wh=3826*2049)

### 2.3.4 刚性事务与柔性事务

刚性事务，遵循 ACID 原则，具有强一致性。比如，数据库事务。柔性事务，其实就是根据不同的业务场景使用不同的方法实现最终一致性，也就是说我们可以根据业务的特性做部分取舍，容忍一定时间内的数据不一致

BASE 理论包括基本可用（Basically Available）、柔性状态（Soft State）和最终一致性（Eventual Consistency）。基本可用：分布式系统出现故障的时候，允许损失一部分功能的可用性，保证核心功能可用。比如，某些电商 618 大促的时候，会对一些非核心链路的功能进行降级处理。柔性状态：在柔性事务中，允许系统存在中间状态，且这个中间状态不会影响系统整体可用性。比如，数据库读写分离，写库同步到读库（主库同步到从库）会有一个延时，其实就是一种柔性状态。最终一致性：事务在操作过程中可能会由于同步延迟等问题导致不一致，但最终状态下，所有数据都是一致的。

三种分布式事务实现方式，二阶段提交、三阶段提交方法，遵循的是 ACID 原则，而消息最终一致性方案遵循的就是 BASE 理论

![img](https://static001.geekbang.org/resource/image/3d/13/3dabbddf3eab0297c2d154245ccb3c13.png?wh=795*777)

## 2.4 分布式锁

与普通锁不同的是，分布式锁是指分布式环境下，系统部署在多个机器中，实现多进程分布式互斥的一种锁。为了保证多个进程能看到锁，锁被存在公共存储（比如 Redis、Memcached、数据库等三方存储中），以实现多个进程并发访问同一个临界资源，同一时刻只有一个进程可访问共享资源，确保数据的一致性

分布式锁的三种实现方法及对比

### 2.4.1 基于数据库实现分布式锁

这里的数据库指的是关系型数据库

实现分布式锁最直接的方式通过数据库进行实现，首先创建一张表用于记录共享资源信息，然后通过操作该表的数据来实现共享资源信息的修改。

但数据库需要落到硬盘上，频繁读取数据库会导致 IO 开销大，因此这种分布式锁适用于并发量低，对性能要求低的场景

基于数据库实现分布式锁比较简单，绝招在于创建一张锁表，为申请者在锁表里建立一条记录，记录建立成功则获得锁，消除记录则释放锁。该方法依赖于数据库，主要有两个缺点：单点故障问题。一旦数据库不可用，会导致整个系统崩溃。死锁问题。数据库锁没有失效时间，未获得锁的进程只能一直等待已获得锁的进程主动释放锁。倘若已获得共享资源访问权限的进程突然挂掉、或者解锁操作失败，使得锁记录一直存在数据库中，无法被删除，而其他进程也无法获得锁，从而产生死锁现象

### 2.4.2 基于缓存实现分布式锁

所谓基于缓存，也就是说把数据存放在计算机内存中，不需要写入磁盘，减少了 IO 读写

Redis 通过队列来维持进程访问共享资源的先后顺序。Redis 锁主要基于 setnx 函数实现分布式锁，当进程通过 setnx 函数返回 1 时，表示已经获得锁。排在后面的进程只能等待前面的进程主动释放锁，或者等到时间超时才能获得锁。

基于缓存实现的分布式锁的优势表现在以下几个方面： 

1. 性能更好。数据被存放在内存，而不是磁盘，避免了频繁的 IO 操作。
2. 很多缓存可以跨集群部署，避免了单点故障问题。使用方便。
3. 很多缓存服务都提供了可以用来实现分布式锁的方法，比如 Redis 的 setnx 和 delete 方法等。可以直接设置超时时间（例如 expire key timeout）来控制锁的释放，因为这些缓存服务器一般支持自动删除过期数据。

### 2.4.3 基于 ZooKeeper 实现分布式锁

ZooKeeper 基于树形数据存储结构实现分布式锁，来解决多个进程同时访问同一临界资源时，数据的一致性问题。

ZooKeeper 的树形数据存储结构主要由 4 种节点构成： 

1. 持久节点（PERSISTENT）。这是默认的节点类型，一直存在于 ZooKeeper 中。
2. 持久顺序节点（PERSISTENT_SEQUENTIAL）。在创建节点时，ZooKeeper 根据节点创建的时间顺序对节点进行编号命名。
3. 临时节点（EPHEMERAL）。当客户端与 Zookeeper 连接时临时创建的节点。与持久节点不同，当客户端与 ZooKeeper 断开连接后，该进程创建的临时节点就会被删除。
4. 临时顺序节点（EPHEMERAL_SEQUENTIAL）。就是按时间顺序编号的临时节点。

还是以电商售卖吹风机的场景为例。假设用户 A、B、C 同时在 11 月 11 日的零点整提交了购买吹风机的请求，ZooKeeper 会采用如下方法来实现分布式锁：在与该方法对应的持久节点 shared_lock 的目录下，为每个进程创建一个临时顺序节点。如下图所示，吹风机就是一个拥有 shared_lock 的目录，当有人买吹风机时，会为他创建一个临时顺序节点。每个进程获取 shared_lock 目录下的所有临时节点列表，注册 Watcher，用于监听子节点变更的信息。当监听到自己的临时节点是顺序最小的，则可以使用共享资源。每个节点确定自己的编号是否是 shared_lock 下所有子节点中最小的，若最小，则获得锁。例如，用户 A 的订单最先到服务器，因此创建了编号为 1 的临时顺序节点 LockNode1。该节点的编号是持久节点目录下最小的，因此获取到分布式锁，可以访问临界资源，从而可以购买吹风机。若本进程对应的临时节点编号不是最小的，则分为两种情况：本进程为读请求，如果比自己序号小的节点中有写请求，则等待；本进程为写请求，如果比自己序号小的节点中有请求，则等待。

使用 ZooKeeper 实现的分布式锁，可以解决前两种方法提到的各种问题，比如单点故障、不可重入、死锁等问题。但该方法实现较复杂，且需要频繁地添加和删除节点，所以性能不如基于缓存实现的分布式锁。

![img](https://static001.geekbang.org/resource/image/da/b9/daea1d41a6b72c288d292adf45ad4bb9.jpg?wh=2782*1142)

ZooKeeper 分布式锁的可靠性最高，有封装好的框架，很容易实现分布式锁的功能，并且几乎解决了数据库锁和缓存式锁的不足，因此是实现分布式锁的首选方法。

为了确保分布式锁的可用性，我们在设计时应考虑到以下几点：互斥性，即在分布式系统环境下，对于某一共享资源，需要保证在同一时间只能一个线程或进程对该资源进行操作。具备锁失效机制，防止死锁。即使出现进程在持有锁的期间崩溃或者解锁失败的情况，也能被动解锁，保证后续其他进程可以获得锁。可重入性，即进程未释放锁时，可以多次访问临界资源。有高可用的获取锁和释放锁的功能，且性能要好

那如何解决羊群效应问题？具体方法可以分为以下三步。在与该方法对应的持久节点的目录下，为每个进程创建一个临时顺序节点。每个进程获取所有临时节点列表，对比自己的编号是否最小，若最小，则获得锁。若本进程对应的临时节点编号不是最小的，则注册 Watcher，监听自己的上一个临时顺序节点，当监听到该节点释放锁后，获取锁

![img](https://static001.geekbang.org/resource/image/0a/87/0ac5f2ac38f1eb46cf5ad681d5153887.png?wh=1979*2119)

## 2.5 分布式与人工智能

核心：数据、模型（算法）和算力

分布式主要解决算力问题

### 2.5.1 数据处理

数据处理又称数据预处理，是指通过数据统计、数据集成、数据清理、数据规约、数据变换等方法，对数据缺失、数据噪声、数据冗余、多数据源等问题进行处理以得到高质量数据，为模型训练提供高质量输入，是人工智能不可缺少的环节。

1. 数据统计：数据统计是数据预处理的第一步，其范围、规模、方式等会直接影响数据分析的结果。常见的统计特征有最大值、最小值、均值、中位数、方差、标准差等
2. 数据集成：需要数据集成来将多个数据源的数据整合到一起，以保证数据结构、属性的一致性，并去除冗余数据，方便后续分析
3. 数据规约：数据规约通常指通过主成分分析法 (Principal Component Analysis，PCA)、小波变换 (Wavelet Transform，WT) 等方法去除重复特征及不重要的特征，从而减少数据的维度或者数据量，降低问题复杂度，同时不影响后面训练的结果
4. 数据变换：数据变化是指通过标准化、离散化和分层化等方法对数据进行集成、清理、规约等操作，使得数据更加一致、更加容易被模型处理。数据变换方法主要有数据标准化、数据离散化和数据泛化三类

业界已经有很多大数据处理软件，比如分布式计算框架 MapReduce、Spark，分布式存储框架 HDFS、HBase 等，来进行分布式数据处理

### 2.5.2 分布式模型训练

数据分布式训练

模型分布式训练

混合模型分布式训练

![img](https://static001.geekbang.org/resource/image/3b/43/3b626dcb58d4cec1b405e3c24eed0943.jpg?wh=3326*2456)

# 3 分布式资源管理与负载调度

## 3.1 分布式体系结构之集中式结构

集中式结构就是，由一台或多台服务器组成中央服务器，系统内的所有数据都存储在中央服务器中，系统内所有的业务也均先由中央服务器处理。多个节点服务器与中央服务器连接，并将自己的信息汇报给中央服务器，由中央服务器统一进行资源和任务调度：中央服务器根据这些信息，将任务下达给节点服务器；节点服务器执行任务，并将结果反馈给中央服务器。

特点，就是部署结构简单

经典集中式结构：Google Borg

![img](https://static001.geekbang.org/resource/image/89/a3/89fbc9c74143c3b9e7cc50908ecb58a3.jpg?wh=3692*2318)

## 3.2 分布式体系结构之非集中式结构

在非集中式结构中，服务的执行和数据的存储被分散到不同的服务器集群，服务器集群间通过消息传递进行通信和协调

### 3.2.1 Akka 集群

Akka 是一个开发库和运行环境，用于构建可扩展的、弹性的、快速响应的应用程序。Akka 框架是基于 Actor 模型实现的，Actor 模型是一个封装了状态和行为的对象，它接收消息并基于该消息执行计算。Actor 之间通信的唯一机制就是消息传递，每个 Actor 都有自己的 MailBox。

### 3.2.2 Redis 集群

Redis 是一个开源的、包含多种数据结构的高性能 Key-value 数据库，主要有以下特征：支持多种数据结构，包括字符串（String）、散列（Hash）、列表（List）、集合（Set）、有序集合（Sorted Set）等；支持数据的持久化和备份。数据可以保存在磁盘中，下次直接加载使用，且可以采用主从模式（Master/Slave）进行数据备份。基于内存运行，具有极高的性能。

### 3.2.3 Cassandra 集群

与 Redis 类似，Cassandra 也支持数据的分布式存储和操作。因此，Cassandra 的集群架构与数据分片存储方案，与 Redis 集群类似。

![img](https://static001.geekbang.org/resource/image/be/88/be7d5a5b7e435f34fc60b5bf9a54a988.jpg?wh=1232*734)

![img](https://static001.geekbang.org/resource/image/00/67/00b66db06581dd66e5f33f8100d78f67.png?wh=1234*1502)

## 3.3 分布式调度架构之单体调度

分布式系统中的单体调度是指，一个集群中只有一个节点运行调度进程，该节点对集群中的其他节点具有访问权限，可以对其他节点的资源信息、节点状态等进行统一管理，同时根据用户下发的任务对资源的需求，在调度器中进行任务与资源匹配，然后根据匹配结果将任务指派给合适的节点,Google Borg、Kubernetes都是

Borg 调度算法Borg 调度算法的核心思想是“筛选可行，评分取优”，具体包括两个阶段：可行性检查，找到一组可以运行任务的机器（即上图中的 Borglet）；评分，从可行的机器中选择一个合适的机器（即上图中的 Borglet）

## 3.4 分布式调度架构之两层调度

在单体调度架构中，中央服务器的单点瓶颈问题，会限制调度的效率和支持的任务类型。中央服务器的性能会限制调度的效率

把资源和任务分开调度，也就是说一层调度器只负责资源管理和分配，另外一层调度器负责任务与资源的匹配

两层调度器中的第一层调度器仍是一个经简化的中央调度器，通常放在分布式集群管理系统中，而第二层调度则是由各个应用程序框架完成。两层调度器的职责分别是：第一层调度器负责管理资源并向框架分配资源，第二层调度器接收分布式集群管理系统中第一层调度器分配的资源，然后根据任务和接收到的资源进行匹配。

典型代表是 Apache Mesos 和 Hadoop YARN

资源分配算法：我们看看最大最小公平算法。这是一种在兼顾公平的前提下，尽可能让更多人满意的资源分配算法。为什么这么说呢？因为这个算法有 3 个主要原则：按照用户对资源需求量递增的顺序进行空闲资源分配；不存在用户得到的资源超过自己需求的情况；对于分配的资源不满足需求的用户，所获得的资源是相等的

最大最小公平算法适用于单一类型的资源分配场景，而主导资源公平算法适用于多种类型资源混合的场景。并且，最大最小公平算法从公平的角度出发，为每个用户分配不多于需求量的资源；而主导资源公平算法从任务出发，目的在于尽量充分利用资源使得能够执行的任务越多越好。

![img](https://static001.geekbang.org/resource/image/0c/30/0cd6814755cfa921f5366c71cb224330.png?wh=1249*882)

### 3.4 分布时调度结构之共享状态调度

 这种调度架构在支持多种任务类型的同时，还能拥有全局的资源状态信息。要做到这一点，这种调度架构的多个调度器需要共享集群状态，包括资源状态和任务状态等。因此，这种调度架构，我们称之为共享状态调度器

共享状态调度架构为了提供高可用性和可扩展性，将集群状态之外的功能抽出来作为独立的服务

![img](https://static001.geekbang.org/resource/image/65/df/651b95130de61792b85d958fcc9c7cdf.png?wh=1235*832)

## 3.5 分布式事务与分布式锁的相关问题

# 4 分布式计算技术

## 4.1 分布式计算模式MapReduce

核心是分而治之

apReduce 主要包括以下三种组件：Master，也就是 MRAppMaster，该模块像一个大总管一样，独掌大权，负责分配任务，协调任务的运行，并为 Mapper 分配 map() 函数操作、为 Reducer 分配 reduce() 函数操作。Mapper worker，负责 Map 函数功能，即负责执行子任务。Reducer worker，负责 Reduce 函数功能，即负责汇总各个子任务的结果。

整个 MapReduce 的工作流程主要可以概括为 5 个阶段，即：Input（输入）、Splitting（拆分）、Mapping（映射）、Reducing（化简）以及 Final Result（输出）

## 4.2 分布式计算模式 Stream

处理流数据任务的计算模式，在分布式领域中叫作 Stream

流数据的特征主要包括以下 4 点：数据如流水般持续、快速地到达；海量数据规模，数据量可达到 TB 级甚至 PB 级；对实时性要求高，随着时间流逝，数据的价值会大幅降低；数据顺序无法保证，也就是说系统无法控制将要处理的数据元素的顺序

### 4.2.1 Stream 工作原理

流计算强调的是实时性，数据一旦产生就会被立即处理，当一条数据被处理完成后，会序列化存储到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理，而不是像 MapReduce 那样，等到缓存写满才开始处理、传输。为了保证数据的实时性，在流计算中，不会存储任何数据，就像水流一样滚滚向前

主流的划分方式是将其分为如下 3 类：商业级的流计算平台，比如 IBM 的 InfoSphere Streams 和 TIBCO 的 StreamBase。InfoSphere Streams 支持同时分析多种数据类型并实时执行复杂计算。StreamBase 是一个用于实时分析的软件，可以快速构建分析系统，即时做出决策。StreamBase 可以为投资银行、对冲基金、政府机构等提供实时数据分析服务。开源流计算框架，典型代表是 Apache Storm（由 Twitter 开源）和 S4（由 Yahoo 开源）。Storm 是一个分布式的、容错的实时计算系统，可以持续进行实时数据流处理，也可以用于分布式 RPC。S4 是一个通用的、分区容错的、可扩展的、可插拔的分布式流式系统。这些开源的分布式流计算系统由于具备开源代码，因此比较适合开发人员将其搭建在自身业务系统中。各大公司根据自身业务特点而开发的流计算框架，比如 Facebook 的 Puma、百度的 Dstream（旨在处理有向无环的数据流）、淘宝的银河流数据处理平台（一个通用的、低延迟、高吞吐、可复用的流数据实时计算系统）

### 4.2.2 Storm 的工作原理

开源的流计算框架

![img](https://static001.geekbang.org/resource/image/53/2c/53f87f8de7f10562db5ce74b748bce2c.jpg?wh=1531*802)

![img](https://static001.geekbang.org/resource/image/fb/5e/fbb7a3acd35fd34bdb7727ce3a0caf5e.png?wh=1074*958)

## 4.3 分布式计算模式 Actor

Actor 模型，代表一种分布式并行计算模型。这种模型有自己的一套规则，规定了 Actor 的内部计算逻辑，以及多个 Actor 之间的通信规则。在 Actor 模型里，每个 Actor 相当于系统中的一个组件，都是基本的计算单元

Actor 模型的三要素是状态、行为和消息

## 4.4 分布式计算模式流水线

![img](https://static001.geekbang.org/resource/image/43/5e/4329251192d6d77935e0de98ff8ebf5e.png?wh=1320*875)

# 5 分布式通信技术

## 5.1 远程调用

本地过程调用（Local Procedure Call，LPC），是指同一台机器上运行的不同进程之间的互相通信，即在多进程操作系统中，运行的不同进程之间可以通过 LPC 进行函数调用。

远程过程调用（Remote Procedure Call，RPC），是指不同机器上运行的进程之间的相互通信，某一机器上运行的进程在不知道底层通信细节的情况下，就像访问本地服务一样，去调用远程机器上的服务

注册中心

## 5.2 发布订阅

异步通信模式

### 5.2.1 发布订阅

发布订阅的三要素是生产者、消费者和消息中心

Kafka 发布订阅原理及工作机制

![img](https://static001.geekbang.org/resource/image/8e/f2/8e24456abc8dc8c3e9abee53b5bedff2.png?wh=1527*926)

## 5.3 消息队列

先进先出，结构体定义消息体

![img](https://static001.geekbang.org/resource/image/25/ef/258dbc95914e8971b584400c528a00ef.png?wh=1230*918)

# 6 分布式数据存储

## 6.1 CAP理论

一致性、可用性以及分区容错性不能同时存在。所以在实际场景中需要选择

## 6.2 分布式数据存储三要素

顾客、导购与货架

导购：分片技术

按照数据特征进行了数据分片，当然，还有其他很多数据分片的方案。比如，按照数据范围，采用哈希映射、一致性哈希环等对数据划分

## 6.3 数据分布方式之哈希和一致性哈希

## 6.4 分布式数据复制技术

同步复制技术

异步复制技术

# 7 分布式可靠

## 7.1 负载均衡

## 7.2 流量控制

漏斗策略

令牌桶策略

## 7.3 故障隔离

线程级隔离

进程级隔离

资源隔离

集群隔离

![img](https://static001.geekbang.org/resource/image/72/fc/7233f358e26052b4e9f58f67bb514afc.jpg?wh=3760*3744)

![img](https://static001.geekbang.org/resource/image/b5/6b/b5e25369e1645514e9cf8c2d2a52866b.png?wh=1164*924)

## 7.4 故障恢复

物理故障

软件故障

![img](https://static001.geekbang.org/resource/image/55/c7/552788a0f75b7a529439fd9ba04dc0c7.png?wh=2159*1755)

## 7.5 判断并解决网络分区问题

心跳